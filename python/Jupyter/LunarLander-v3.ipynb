{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0847f5-1fa3-4b56-8c5f-5295f8e5b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import numpy as np\n",
    "\n",
    "class ShapedLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Hover penalty (√ßok sabit kalmasƒ±n)\n",
    "        if y > 0.15 and velocity_mag < 0.05:\n",
    "            reward -= 0.02   # k√º√ß√ºk ceza, -4 √ßok fazlaydƒ±\n",
    "\n",
    "        # Fuel penalty (ger√ßek√ßi yakƒ±t kullanƒ±mƒ±)\n",
    "        if action == 2:      # main engine\n",
    "            reward -= 0.03\n",
    "        elif action in [1,3]: # side engines\n",
    "            reward -= 0.015\n",
    "\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "# Environment olu≈ütur\n",
    "normal_env = gym.make(\"LunarLander-v3\")\n",
    "rnormal_env = gym.make(\"LunarLander-v3\",render_mode=\"human\")\n",
    "shaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "rshaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\",render_mode=\"human\"))\n",
    "env = shaped_env\n",
    "model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,        # PPO i√ßin tipik\n",
    "        n_steps=2048,              # rollout buffer length\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "    )\n",
    "# Eƒüitilmi≈ü agenti y√ºkle (veya halihazƒ±rda model deƒüi≈ükenindeyse onu kullanabilirsin)\n",
    "#model = PPO.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_51200_model\", env)\n",
    "model.learn(total_timesteps=50000,reset_num_timesteps=True)\n",
    "\n",
    "obs, _ = env.reset(seed=11)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Agent aksiyonu tahmin eder (deterministic=True, yani rastgelelik yok)\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Ortamda aksiyonu uygula\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    # Oyunun bitip bitmediƒüini kontrol et\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Render ile ekranda g√∂ster\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "size = model.num_timesteps\n",
    "    # Hemen save edin\n",
    "model.save(rf\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\PPO\\fresh_{size}_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddcd52d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'exploration_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Modeli y√ºkledikten sonra\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToplam eƒüitim adƒ±mƒ±: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.num_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'PPO' object has no attribute 'exploration_rate'"
     ]
    }
   ],
   "source": [
    "print(f\"Model epsilon: {model.exploration_rate}\")\n",
    "\n",
    "# Modeli y√ºkledikten sonra\n",
    "print(f\"Toplam eƒüitim adƒ±mƒ±: {model.num_timesteps}\")\n",
    "print(f\"Exploration rate: {model.exploration_rate}\")\n",
    "\n",
    "# Eƒüer num_timesteps d√º≈ü√ºkse, model yeterince eƒüitilmemi≈ü olabilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3239d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model timesteps: 51200\n",
      "‚úÖ Model hala bellekte\n",
      "Acil durum save yapƒ±ldƒ±!\n"
     ]
    }
   ],
   "source": [
    "# Model deƒüi≈ükeni hala var mƒ± kontrol edin\n",
    "try:\n",
    "    print(f\"Model timesteps: {model.num_timesteps}\")\n",
    "    print(\"‚úÖ Model hala bellekte\")\n",
    "    size = model.num_timesteps\n",
    "    # Hemen save edin\n",
    "    model.save(rf\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_{size}_model\")\n",
    "    print(\"Acil durum save yapƒ±ldƒ±!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Model deƒüi≈ükeni kaybolmu≈ü\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model bozulmu≈ü: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881de08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Ba≈ülangƒ±√ß timesteps: 0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.3     |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    fps             | 2064     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.2        |\n",
      "|    ep_rew_mean          | -214        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1474        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009106861 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00122    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 611         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.96e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98           |\n",
      "|    ep_rew_mean          | -193         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1319         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062277545 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0336      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.9        |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1270        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010755452 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0164     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 482         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    value_loss           | 1e+03       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1250        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010967029 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 247         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 517         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1232        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009512791 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.000628    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 241         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00843    |\n",
      "|    value_loss           | 388         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1218        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009535676 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.00975    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 196         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    value_loss           | 619         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1212        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013940337 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.000332    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 137         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 234         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1209        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010422844 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.0647     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 445         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -86.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1205         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112189315 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00654     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 243          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00683     |\n",
      "|    value_loss           | 355          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | -86.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1201        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010112261 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.00848     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 137        |\n",
      "|    ep_rew_mean          | -78.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1198       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00758583 |\n",
      "|    clip_fraction        | 0.0298     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.00552    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 390        |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00364   |\n",
      "|    value_loss           | 530        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 146         |\n",
      "|    ep_rew_mean          | -69.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1197        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008200465 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.003      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    value_loss           | 289         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | -63.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1193         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058514094 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0473       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 144          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    value_loss           | 345          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | -60.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005343602 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.0744      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 90.8        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 184          |\n",
      "|    ep_rew_mean          | -56.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1186         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052190535 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 108          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00492     |\n",
      "|    value_loss           | 239          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 196         |\n",
      "|    ep_rew_mean          | -53         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1181        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008157302 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 210          |\n",
      "|    ep_rew_mean          | -45.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1180         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057930076 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0288       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 116          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00913     |\n",
      "|    value_loss           | 240          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 226         |\n",
      "|    ep_rew_mean          | -43.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1178        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007274982 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00744    |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 238         |\n",
      "|    ep_rew_mean          | -40.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1179        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005812675 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 255         |\n",
      "|    ep_rew_mean          | -40.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1178        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007189046 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 93.6        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 147         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 270        |\n",
      "|    ep_rew_mean          | -38        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1178       |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00831664 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.0675     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 101        |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00677   |\n",
      "|    value_loss           | 251        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 287          |\n",
      "|    ep_rew_mean          | -41.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1178         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118731605 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 31.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 304         |\n",
      "|    ep_rew_mean          | -41.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1179        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008874142 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 318         |\n",
      "|    ep_rew_mean          | -41.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1180        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011502445 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.3        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "Final timesteps: 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olcan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-hovering model performansƒ±: -306.54\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "class AntiHoveringLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.hovering_count = 0\n",
    "        self.step_count = 0\n",
    "        self.altitude_history = []\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.hovering_count = 0\n",
    "        self.step_count = 0\n",
    "        self.altitude_history = []\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy, angle, angular_vel, leg1, leg2 = obs\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Altitude history tutla\n",
    "            self.altitude_history.append(y)\n",
    "            if len(self.altitude_history) > 20:\n",
    "                self.altitude_history.pop(0)\n",
    "            \n",
    "            # AGRESIF HOVERING DETECTION\n",
    "            is_hovering = (\n",
    "                y > 0.15 and                    # Yeterince y√ºksekte\n",
    "                velocity_mag < 0.08 and         # √áok yava≈ü\n",
    "                abs(vx) < 0.05 and             # Yatay hareket az\n",
    "                abs(vy) < 0.05                 # Dikey hareket az\n",
    "            )\n",
    "            \n",
    "            if is_hovering:\n",
    "                self.hovering_count += 1\n",
    "                \n",
    "                # Hovering s√ºresine g√∂re artan ceza\n",
    "                base_penalty = -0.1\n",
    "                time_penalty = -0.02 * self.hovering_count\n",
    "                \n",
    "                # Uzun s√ºre hovering = episode sonlandƒ±r\n",
    "                if self.hovering_count > 30:  # 30 step = yakla≈üƒ±k 1 saniye\n",
    "                    reward = -200  # B√ºy√ºk ceza\n",
    "                    terminated = True\n",
    "                    info['hovering_termination'] = True\n",
    "                else:\n",
    "                    reward += base_penalty + time_penalty\n",
    "                \n",
    "            else:\n",
    "                self.hovering_count = max(0, self.hovering_count - 2)  # Yava≈ü√ßa azalt\n",
    "            \n",
    "            # FUEL AGRESIF CEZA\n",
    "            if action == 2:  # Ana motor\n",
    "                reward -= 0.03\n",
    "            elif action in [1, 3]:  # Yan motorlar\n",
    "                reward -= 0.015\n",
    "            \n",
    "            # PROGRESS ZORLAMA\n",
    "            target_x = 0.0\n",
    "            distance_to_target = abs(x - target_x)\n",
    "            \n",
    "            # √áok uzak kalƒ±rsa ceza\n",
    "            if distance_to_target > 0.8:\n",
    "                reward -= 0.05\n",
    "            \n",
    "            # A≈üaƒüƒ± gitmeyi te≈üvik et\n",
    "            if len(self.altitude_history) >= 10:\n",
    "                recent_avg = np.mean(self.altitude_history[-10:])\n",
    "                older_avg = np.mean(self.altitude_history[-20:-10]) if len(self.altitude_history) >= 20 else recent_avg\n",
    "                \n",
    "                if recent_avg < older_avg:  # A≈üaƒüƒ± iniyor\n",
    "                    reward += 0.05\n",
    "                elif recent_avg > older_avg:  # Yukarƒ± √ßƒ±kƒ±yor\n",
    "                    reward -= 0.03\n",
    "            \n",
    "            # MAX EPISODE LIMIT\n",
    "            if self.step_count > 500:  # Normal limit 1000, biz 500 yapalƒ±m\n",
    "                reward -= 0.1\n",
    "                if self.step_count > 600:\n",
    "                    terminated = True\n",
    "                    info['time_limit_termination'] = True\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "class ShapedLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Hovering penalty\n",
    "            if y > 0.1 and velocity_mag < 0.05:\n",
    "                reward -= 0.02\n",
    "            \n",
    "            # Fuel penalty\n",
    "            if action == 2:  # Main engine\n",
    "                reward -= 0.01\n",
    "            elif action in [1, 3]:  # Side engines\n",
    "                reward -= 0.005\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# 25K modelinin √ºzerine shaped env ile eƒüitim\n",
    "def train_on_shaped_env():\n",
    "    # Shaped environment olu≈ütur\n",
    "    shaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # 25K modelini y√ºkle\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_100000_model\", env=shaped_env)\n",
    "    \n",
    "    print(f\"Ba≈ülangƒ±√ß timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # 25K step daha eƒüit\n",
    "    model.learn(total_timesteps=200000, reset_num_timesteps=False)  # 10K + 25K = 35K\n",
    "    \n",
    "    print(f\"Final timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # Test\n",
    "    mean_reward, _ = evaluate_policy(model, shaped_env, n_eval_episodes=5)\n",
    "    print(f\"Shaped env performansƒ±: {mean_reward:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    model.save(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    return model\n",
    "\n",
    "def test_anti_hovering():\n",
    "    \"\"\"Anti-hovering environment'larƒ± test et\"\"\"\n",
    "    \n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    \n",
    "    # 1. Normal shaped env (sizin mevcut)\n",
    "    shaped_env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # 2. Force descent env\n",
    "    force_env = ForceDescentLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    print(\"=== ANTI-HOVERING TEST ===\")\n",
    "    \n",
    "    # Mevcut modelinizi test edin\n",
    "    model_path = r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\"\n",
    "    \n",
    "    try:\n",
    "        # Shaped env ile test\n",
    "        model1 = DQN.load(model_path, env=shaped_env)\n",
    "        perf1, _ = evaluate_policy(model1, shaped_env, n_eval_episodes=3)\n",
    "        print(f\"Anti-hovering env: {perf1:.2f}\")\n",
    "        \n",
    "        # Force env ile test\n",
    "        model2 = DQN.load(model_path, env=force_env)\n",
    "        perf2, _ = evaluate_policy(model2, force_env, n_eval_episodes=3)\n",
    "        print(f\"Force descent env: {perf2:.2f}\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Model y√ºklenemedi, yeni model eƒüitiliyor...\")\n",
    "        \n",
    "        # Anti-hovering ile eƒüitim\n",
    "        model = DQN(\"MlpPolicy\", shaped_env, verbose=1)\n",
    "        model.learn(total_timesteps=20000)\n",
    "        \n",
    "        perf, _ = evaluate_policy(model, shaped_env, n_eval_episodes=5)\n",
    "        print(f\"Yeni anti-hovering model: {perf:.2f}\")\n",
    "        \n",
    "        model.save(\"anti_hovering_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    force_env.close()\n",
    "class ForceDescentLunarLander(gym.Wrapper):\n",
    "    \"\"\"Daha da agresif - ini≈ü zorlamasƒ±\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.step_count = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            prev_y = self.prev_obs[1]\n",
    "            \n",
    "            # BASIT AMA ETKƒ∞Lƒ∞: Step sayƒ±sƒ±na g√∂re ceza\n",
    "            altitude_penalty = -0.01 * self.step_count * y\n",
    "            reward += altitude_penalty\n",
    "            \n",
    "            # A≈üaƒüƒ± gitme √∂d√ºl√º\n",
    "            if y < prev_y:  # A≈üaƒüƒ± iniyor\n",
    "                reward += 0.1 * (prev_y - y)\n",
    "            \n",
    "            # Yukarƒ± gitme cezasƒ±\n",
    "            elif y > prev_y:  # Yukarƒ± √ßƒ±kƒ±yor\n",
    "                reward -= 0.2 * (y - prev_y)\n",
    "            \n",
    "            # Y√ºkseklik threshold'u\n",
    "            if y > 1.0 and self.step_count > 100:\n",
    "                reward -= 1.0  # √áok y√ºksekte √ßok uzun s√ºre\n",
    "            \n",
    "            # Episode sonlandƒ±rma\n",
    "            if self.step_count > 400:\n",
    "                terminated = True\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "def fresh_shaped_training():\n",
    "    \"\"\"Sƒ±fƒ±rdan shaped environment ile eƒüitim\"\"\"\n",
    "    \n",
    "    print(\"\\n=== SIFIRDAN SHAPED Eƒûƒ∞Tƒ∞Mƒ∞ ===\")\n",
    "    \n",
    "    shaped_env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # Yeni model olu≈ütur\n",
    "    fresh_model = DQN(\"MlpPolicy\", shaped_env, verbose=1)\n",
    "    \n",
    "    # Eƒüit\n",
    "    fresh_model.learn(total_timesteps=150000)\n",
    "    \n",
    "    # Test\n",
    "    fresh_perf, _ = evaluate_policy(fresh_model, shaped_env, n_eval_episodes=5)\n",
    "    print(f\"Sƒ±fƒ±rdan shaped model performansƒ±: {fresh_perf:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    fresh_model.save(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_shaped_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    return fresh_model, fresh_perf\n",
    "\n",
    "def quick_fix():\n",
    "    \"\"\"Hƒ±zlƒ± hovering d√ºzeltmesi\"\"\"\n",
    "    \n",
    "    # Environment\n",
    "    env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # Model y√ºkle (yeni model olu≈üturmaya gerek yok)\n",
    "    #model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\PPO\\final_anti_hovering_model\", env=env)\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,        # PPO i√ßin tipik\n",
    "        n_steps=2048,              # rollout buffer length\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "    )\n",
    "\n",
    "    print(f\"Ba≈ülangƒ±√ß timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    model.learn(total_timesteps=50000, reset_num_timesteps=False)\n",
    "    \n",
    "    print(f\"Final timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # Test\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    perf, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"Anti-hovering model performansƒ±: {perf:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    model.save(\"PPO\\AntiHoveringLunarLander_model\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# √áalƒ±≈ütƒ±r\n",
    "model = quick_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "151fa76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE√áENEKLER:\n",
      "1. nuclear_option()      - EN EXTREME √á√ñZ√úM\n",
      "2. simple_brutal_fix()   - BASIT BRUTAL\n",
      "3. debug_hovering()      - MEVCUT MODELƒ∞ DEBUG ET\n",
      "üîç HOVERING DEBUG\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step  0: Action=2, Y=1.386, Vel=0.739, Reward=1.31\n",
      "Step  1: Action=2, Y=1.375, Vel=0.718, Reward=2.40\n",
      "Step  2: Action=2, Y=1.363, Vel=0.718, Reward=0.32\n",
      "Step  3: Action=2, Y=1.352, Vel=0.708, Reward=1.12\n",
      "Step  4: Action=2, Y=1.341, Vel=0.672, Reward=3.68\n",
      "Step  5: Action=2, Y=1.331, Vel=0.646, Reward=2.55\n",
      "Step  6: Action=2, Y=1.322, Vel=0.633, Reward=1.16\n",
      "Step  7: Action=2, Y=1.312, Vel=0.644, Reward=-1.20\n",
      "Step  8: Action=2, Y=1.303, Vel=0.626, Reward=1.61\n",
      "Step  9: Action=2, Y=1.294, Vel=0.639, Reward=-1.37\n",
      "Step 10: Action=2, Y=1.285, Vel=0.647, Reward=-0.85\n",
      "Step 11: Action=2, Y=1.277, Vel=0.646, Reward=-0.01\n",
      "Step 12: Action=1, Y=1.268, Vel=0.654, Reward=-0.39\n",
      "Step 13: Action=2, Y=1.260, Vel=0.668, Reward=-1.12\n",
      "Step 14: Action=1, Y=1.251, Vel=0.673, Reward=0.27\n",
      "Step 15: Action=2, Y=1.242, Vel=0.672, Reward=0.60\n",
      "Step 16: Action=1, Y=1.232, Vel=0.680, Reward=0.21\n",
      "Step 17: Action=2, Y=1.223, Vel=0.673, Reward=1.45\n",
      "Step 18: Action=2, Y=1.213, Vel=0.675, Reward=0.48\n",
      "Step 19: Action=2, Y=1.205, Vel=0.656, Reward=2.50\n",
      "Step 20: Action=1, Y=1.195, Vel=0.665, Reward=0.21\n",
      "Step 21: Action=2, Y=1.186, Vel=0.648, Reward=2.40\n",
      "Step 22: Action=1, Y=1.177, Vel=0.656, Reward=0.46\n",
      "Step 23: Action=2, Y=1.168, Vel=0.657, Reward=0.88\n",
      "Step 24: Action=2, Y=1.159, Vel=0.648, Reward=1.87\n",
      "Step 25: Action=1, Y=1.150, Vel=0.657, Reward=0.46\n",
      "Step 26: Action=2, Y=1.141, Vel=0.654, Reward=1.39\n",
      "Step 27: Action=1, Y=1.132, Vel=0.663, Reward=0.84\n",
      "Step 28: Action=2, Y=1.124, Vel=0.664, Reward=1.30\n",
      "Step 29: Action=2, Y=1.115, Vel=0.672, Reward=0.60\n",
      "Step 30: Action=2, Y=1.107, Vel=0.665, Reward=2.12\n",
      "Step 31: Action=1, Y=1.098, Vel=0.670, Reward=1.47\n",
      "Step 32: Action=2, Y=1.091, Vel=0.642, Reward=4.02\n",
      "Step 33: Action=2, Y=1.084, Vel=0.613, Reward=2.06\n",
      "Step 34: Action=2, Y=1.077, Vel=0.589, Reward=1.52\n",
      "Step 35: Action=1, Y=1.071, Vel=0.592, Reward=-1.03\n",
      "Step 36: Action=2, Y=1.064, Vel=0.569, Reward=1.29\n",
      "Step 37: Action=2, Y=1.058, Vel=0.549, Reward=0.88\n",
      "Step 38: Action=2, Y=1.053, Vel=0.547, Reward=-0.97\n",
      "Step 39: Action=2, Y=1.048, Vel=0.536, Reward=-0.07\n",
      "Step 40: Action=2, Y=1.044, Vel=0.518, Reward=0.50\n",
      "Step 41: Action=2, Y=1.040, Vel=0.503, Reward=0.21\n",
      "Step 42: Action=2, Y=1.036, Vel=0.491, Reward=-0.18\n",
      "Step 43: Action=2, Y=1.033, Vel=0.489, Reward=-1.30\n",
      "Step 44: Action=2, Y=1.031, Vel=0.480, Reward=-0.74\n",
      "Step 45: Action=1, Y=1.029, Vel=0.476, Reward=-1.16\n",
      "Step 46: Action=2, Y=1.026, Vel=0.459, Reward=-0.02\n",
      "Step 47: Action=2, Y=1.024, Vel=0.445, Reward=-0.38\n",
      "Step 48: Action=2, Y=1.022, Vel=0.419, Reward=0.89\n",
      "Step 49: Action=2, Y=1.021, Vel=0.396, Reward=0.55\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "class ExtremeAntiHoveringLunarLander(gym.Wrapper):\n",
    "    \"\"\"HOVERING = INSTANT DEATH\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.hovering_steps = 0\n",
    "        self.step_count = 0\n",
    "        self.max_altitude_seen = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.hovering_steps = 0\n",
    "        self.step_count = 0\n",
    "        self.max_altitude_seen = obs[1]\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        x, y, vx, vy, angle, angular_vel, leg1, leg2 = obs\n",
    "        velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        # Max altitude g√ºncelle\n",
    "        self.max_altitude_seen = max(self.max_altitude_seen, y)\n",
    "        \n",
    "        # HOVERING DETECTION (√ßok hassas)\n",
    "        is_hovering = (\n",
    "            y > 0.1 and \n",
    "            velocity_mag < 0.1 and \n",
    "            abs(vy) < 0.03\n",
    "        )\n",
    "        \n",
    "        if is_hovering:\n",
    "            self.hovering_steps += 1\n",
    "            \n",
    "            # SADECE 5 STEP HOVERING ƒ∞Zƒ∞N VER\n",
    "            if self.hovering_steps > 5:\n",
    "                reward = -500  # EXTREME PENALTY\n",
    "                terminated = True\n",
    "                info['hovering_death'] = True\n",
    "                print(f\"üö® HOVERING DEATH at step {self.step_count}\")\n",
    "        else:\n",
    "            self.hovering_steps = 0\n",
    "        \n",
    "        # ALTITUDE-BASED EXTREME PENALTIES\n",
    "        if y > 0.5:\n",
    "            reward -= 2.0  # √áok y√ºksekte olmak = b√ºy√ºk ceza\n",
    "        \n",
    "        if y > 0.8:\n",
    "            reward -= 5.0  # A≈üƒ±rƒ± y√ºksekte = √ßok b√ºy√ºk ceza\n",
    "        \n",
    "        # YUKAR √áIKMA = √ñL√úM\n",
    "        if self.prev_obs is not None:\n",
    "            prev_y = self.prev_obs[1]\n",
    "            if y > prev_y + 0.02:  # Yukarƒ± √ßƒ±kƒ±yor\n",
    "                reward -= 10.0\n",
    "                print(f\"‚¨ÜÔ∏è UPWARD MOVEMENT PENALTY: {y:.3f} -> {prev_y:.3f}\")\n",
    "        \n",
    "        # PROGRESS FORCE (a≈üaƒüƒ± gitmeyi zorla)\n",
    "        if self.prev_obs is not None:\n",
    "            prev_y = self.prev_obs[1]\n",
    "            if y < prev_y:  # A≈üaƒüƒ± iniyor\n",
    "                reward += 5.0 * (prev_y - y)  # B√ºy√ºk √∂d√ºl\n",
    "        \n",
    "        # TIME PRESSURE (zamanla artan ceza)\n",
    "        time_penalty = -0.1 * (self.step_count / 100)\n",
    "        reward += time_penalty\n",
    "        \n",
    "        # EPISODE HARD LIMIT\n",
    "        if self.step_count > 200:  # √áok kƒ±sa episode\n",
    "            reward = -1000\n",
    "            terminated = True\n",
    "            info['time_limit_death'] = True\n",
    "        \n",
    "        # ALTITUDE REGRESSION REWARD\n",
    "        altitude_progress = self.max_altitude_seen - y\n",
    "        if altitude_progress > 0:\n",
    "            reward += altitude_progress * 10  # Ne kadar a≈üaƒüƒ± indiyse o kadar √∂d√ºl\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "class ForcedDescentLunarLander(gym.Wrapper):\n",
    "    \"\"\"Ba≈ütan a≈üaƒüƒ± inmeye zorla\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.step_count = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        y = obs[1]  # altitude\n",
    "        \n",
    "        # BASIT BRUTAL APPROACH: Altitude = instant penalty\n",
    "        altitude_penalty = -y * 10  # Her metre y√ºkseklik = -10 reward\n",
    "        reward += altitude_penalty\n",
    "        \n",
    "        # Step penalty\n",
    "        reward -= 0.1\n",
    "        \n",
    "        # Hard time limit\n",
    "        if self.step_count > 150:\n",
    "            terminated = True\n",
    "            reward = -500\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "def nuclear_option():\n",
    "    \"\"\"NUCLEAR OPTION: Sƒ±fƒ±rdan tamamen yeni approach\"\"\"\n",
    "    \n",
    "    print(\"üö® NUCLEAR OPTION: EXTREME ANTI-HOVERING üö®\")\n",
    "    \n",
    "    # En extreme environment\n",
    "    env = ExtremeAntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_100000_model\", env=env)\n",
    "\n",
    "    model.learn(total_timesteps=200000, reset_num_timesteps=False)  # 10K + 25K = 35K\n",
    "    # Yeni model - sƒ±fƒ±rdan ba≈üla\n",
    "    # model = DQN(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     env, \n",
    "    #     verbose=1,\n",
    "    #     learning_rate=1e-3,\n",
    "    #     exploration_initial_eps=1.0,    # Tam exploration\n",
    "    #     exploration_final_eps=0.01,     # Minimum exploration  \n",
    "    #     exploration_fraction=0.5,       # Yarƒ±ya kadar explore\n",
    "    #     target_update_interval=500,     # Daha sƒ±k update\n",
    "    #     train_freq=1,                   # Her step train\n",
    "    #     buffer_size=10000               # K√º√ß√ºk buffer\n",
    "    # )\n",
    "    \n",
    "    print(\"üî• EXTREME TRAINING BA≈ûLIYOR...\")\n",
    "    \n",
    "    # √áok kƒ±sa ama yoƒüun eƒüitim\n",
    "    #model.learn(total_timesteps=100000)\n",
    "    \n",
    "    print(\"‚úÖ EXTREME TRAINING Bƒ∞TTƒ∞\")\n",
    "    \n",
    "    # Test\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    \n",
    "    print(\"üß™ TESTING...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"EXTREME Model Performance: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "    \n",
    "    # Manuel test\n",
    "    print(\"\\nüéÆ MANUEL TEST:\")\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while steps < 300:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        y = obs[1]\n",
    "        print(f\"Step {steps}: Action={action}, Altitude={y:.3f}, Reward={reward:.2f}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Manuel test sonucu: {total_reward:.2f} in {steps} steps\")\n",
    "    \n",
    "    if 'hovering_death' in info:\n",
    "        print(\"üíÄ HOVERING DEATH!\")\n",
    "    elif 'time_limit_death' in info:\n",
    "        print(\"‚è∞ TIME LIMIT DEATH!\")\n",
    "    elif terminated:\n",
    "        print(\"üéØ MISSION COMPLETE!\")\n",
    "    \n",
    "    # Son √ßare kaydet\n",
    "    model.save(\"nuclear_anti_hovering_model\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def simple_brutal_fix():\n",
    "    \"\"\"En basit brutal √ß√∂z√ºm\"\"\"\n",
    "    \n",
    "    print(\"üí• SIMPLE BRUTAL FIX\")\n",
    "    \n",
    "    env = ForcedDescentLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\brutal_model\", env=env)\n",
    "    #model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=100000, reset_num_timesteps=False)\n",
    "    \n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    perf, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"Brutal model: {perf:.2f}\")\n",
    "    \n",
    "    model.save(\"brutal_model2\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# SON √áARE TEST\n",
    "def debug_hovering():\n",
    "    \"\"\"Hovering davranƒ±≈üƒ±nƒ± debug et\"\"\"\n",
    "    \n",
    "    print(\"üîç HOVERING DEBUG\")\n",
    "    \n",
    "    # Mevcut modelinizi y√ºkle\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\", env=env)\n",
    "    \n",
    "    # 10 step takip et\n",
    "    obs, _ = env.reset()\n",
    "    for step in range(50):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        x, y, vx, vy = obs[:4]\n",
    "        velocity = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        print(f\"Step {step:2d}: Action={action}, Y={y:.3f}, Vel={velocity:.3f}, Reward={reward:.2f}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# KULLANIM:\n",
    "print(\"SE√áENEKLER:\")\n",
    "print(\"1. nuclear_option()      - EN EXTREME √á√ñZ√úM\")\n",
    "print(\"2. simple_brutal_fix()   - BASIT BRUTAL\")  \n",
    "print(\"3. debug_hovering()      - MEVCUT MODELƒ∞ DEBUG ET\")\n",
    "\n",
    "# EN EXTREME √á√ñZ√úM:\n",
    "model = debug_hovering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
