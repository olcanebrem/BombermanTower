{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0847f5-1fa3-4b56-8c5f-5295f8e5b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96       |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    fps             | 2109     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.8        |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1504        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007470076 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00248     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 520         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    value_loss           | 1.52e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 96         |\n",
      "|    ep_rew_mean          | -157       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1406       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00430545 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | -0.0182    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 440        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0054    |\n",
      "|    value_loss           | 1.13e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.4         |\n",
      "|    ep_rew_mean          | -148         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1364         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074169384 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.0013       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 196          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00839     |\n",
      "|    value_loss           | 643          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.3        |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1329        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011030921 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.00231    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 493         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00965    |\n",
      "|    value_loss           | 572         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.6         |\n",
      "|    ep_rew_mean          | -130         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1314         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075016054 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -0.000599    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 218          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    value_loss           | 444          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1304        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006898445 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -0.000975   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 186         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    value_loss           | 447         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1295        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007855449 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.0013     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 368         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1284        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010900792 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.000401    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 289         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 533         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 128          |\n",
      "|    ep_rew_mean          | -110         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1276         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041372627 |\n",
      "|    clip_fraction        | 0.00537      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 134          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    value_loss           | 229          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 137          |\n",
      "|    ep_rew_mean          | -108         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1273         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061446237 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 177          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    value_loss           | 279          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 145          |\n",
      "|    ep_rew_mean          | -104         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1268         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027659659 |\n",
      "|    clip_fraction        | 0.00151      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | -0.0797      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 163          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    value_loss           | 291          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 155         |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1262        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007836338 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0724      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00749    |\n",
      "|    value_loss           | 281         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | -107         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1259         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057720486 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.00682      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00451     |\n",
      "|    value_loss           | 346          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 184        |\n",
      "|    ep_rew_mean          | -104       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1256       |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00788091 |\n",
      "|    clip_fraction        | 0.0397     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.298      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.3       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00436   |\n",
      "|    value_loss           | 118        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 191          |\n",
      "|    ep_rew_mean          | -105         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1256         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054604956 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 204         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1252        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003818533 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.0244     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 142         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 214          |\n",
      "|    ep_rew_mean          | -98.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1252         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076687215 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 68.4         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00835     |\n",
      "|    value_loss           | 268          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 229          |\n",
      "|    ep_rew_mean          | -99.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1250         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055375295 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 99.6         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00902     |\n",
      "|    value_loss           | 247          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 242          |\n",
      "|    ep_rew_mean          | -96          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1247         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057720514 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.553        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.1         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00549     |\n",
      "|    value_loss           | 163          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 262         |\n",
      "|    ep_rew_mean          | -92.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1245        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010351128 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 85.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 269         |\n",
      "|    ep_rew_mean          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1244        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006743037 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    value_loss           | 81.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 283         |\n",
      "|    ep_rew_mean          | -96.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1242        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011448426 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 94.8        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 289         |\n",
      "|    ep_rew_mean          | -97         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1242        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010700101 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 74.2        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00946    |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 307        |\n",
      "|    ep_rew_mean          | -96.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1243       |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00912584 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.1       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 110        |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import numpy as np\n",
    "\n",
    "class ShapedLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Hover penalty (çok sabit kalmasın)\n",
    "        if y > 0.15 and velocity_mag < 0.05:\n",
    "            reward -= 0.02   # küçük ceza, -4 çok fazlaydı\n",
    "\n",
    "        # Fuel penalty (gerçekçi yakıt kullanımı)\n",
    "        if action == 2:      # main engine\n",
    "            reward -= 0.1\n",
    "        elif action in [1,3]: # side engines\n",
    "            reward -= 0.015\n",
    "\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "# Environment oluştur\n",
    "normal_env = gym.make(\"LunarLander-v3\")\n",
    "rnormal_env = gym.make(\"LunarLander-v3\",render_mode=\"human\")\n",
    "shaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "rshaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\",render_mode=\"human\"))\n",
    "\n",
    "model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,        # PPO için tipik\n",
    "        n_steps=2048,              # rollout buffer length\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "    )\n",
    "# Eğitilmiş agenti yükle (veya halihazırda model değişkenindeyse onu kullanabilirsin)\n",
    "#model = PPO.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\PPO\\fresh_51200_model\", env)\n",
    "model.learn(total_timesteps=50000,reset_num_timesteps=True)\n",
    "size = model.num_timesteps\n",
    "model.save(rf\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\PPO\\fresh_{size}_model\")\n",
    "rshaped_env.close()\n",
    "\n",
    "obs, _ = rnormal_env.reset(seed=42)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Agent aksiyonu tahmin eder (deterministic=True, yani rastgelelik yok)\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Ortamda aksiyonu uygula\n",
    "    obs, reward, terminated, truncated, _ = rnormal_env.step(action)\n",
    "    \n",
    "    # Oyunun bitip bitmediğini kontrol et\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Render ile ekranda göster\n",
    "    rnormal_env.render()\n",
    "rnormal_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddcd52d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'exploration_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Modeli yükledikten sonra\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToplam eğitim adımı: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.num_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'PPO' object has no attribute 'exploration_rate'"
     ]
    }
   ],
   "source": [
    "print(f\"Model epsilon: {model.exploration_rate}\")\n",
    "\n",
    "# Modeli yükledikten sonra\n",
    "print(f\"Toplam eğitim adımı: {model.num_timesteps}\")\n",
    "print(f\"Exploration rate: {model.exploration_rate}\")\n",
    "\n",
    "# Eğer num_timesteps düşükse, model yeterince eğitilmemiş olabilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3239d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model timesteps: 51200\n",
      "✅ Model hala bellekte\n",
      "Acil durum save yapıldı!\n"
     ]
    }
   ],
   "source": [
    "# Model değişkeni hala var mı kontrol edin\n",
    "try:\n",
    "    print(f\"Model timesteps: {model.num_timesteps}\")\n",
    "    print(\"✅ Model hala bellekte\")\n",
    "    size = model.num_timesteps\n",
    "    # Hemen save edin\n",
    "    model.save(rf\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_{size}_model\")\n",
    "    print(\"Acil durum save yapıldı!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"❌ Model değişkeni kaybolmuş\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model bozulmuş: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881de08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Başlangıç timesteps: 0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.3     |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    fps             | 2064     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.2        |\n",
      "|    ep_rew_mean          | -214        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1474        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009106861 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00122    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 611         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.96e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98           |\n",
      "|    ep_rew_mean          | -193         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1319         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062277545 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0336      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.9        |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1270        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010755452 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0164     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 482         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    value_loss           | 1e+03       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1250        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010967029 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 247         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 517         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1232        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009512791 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.000628    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 241         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00843    |\n",
      "|    value_loss           | 388         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1218        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009535676 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.00975    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 196         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    value_loss           | 619         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1212        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013940337 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.000332    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 137         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 234         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1209        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010422844 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.0647     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 445         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -86.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1205         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112189315 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00654     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 243          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00683     |\n",
      "|    value_loss           | 355          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | -86.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1201        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010112261 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.00848     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 137        |\n",
      "|    ep_rew_mean          | -78.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1198       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00758583 |\n",
      "|    clip_fraction        | 0.0298     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.00552    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 390        |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00364   |\n",
      "|    value_loss           | 530        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 146         |\n",
      "|    ep_rew_mean          | -69.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1197        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008200465 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.003      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    value_loss           | 289         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | -63.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1193         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058514094 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0473       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 144          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    value_loss           | 345          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | -60.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005343602 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.0744      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 90.8        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 184          |\n",
      "|    ep_rew_mean          | -56.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1186         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052190535 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 108          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00492     |\n",
      "|    value_loss           | 239          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 196         |\n",
      "|    ep_rew_mean          | -53         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1181        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008157302 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00898    |\n",
      "|    value_loss           | 192         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 210          |\n",
      "|    ep_rew_mean          | -45.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1180         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057930076 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0288       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 116          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00913     |\n",
      "|    value_loss           | 240          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 226         |\n",
      "|    ep_rew_mean          | -43.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1178        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007274982 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00744    |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 238         |\n",
      "|    ep_rew_mean          | -40.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1179        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005812675 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 255         |\n",
      "|    ep_rew_mean          | -40.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1178        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007189046 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 93.6        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 147         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 270        |\n",
      "|    ep_rew_mean          | -38        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1178       |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00831664 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.0675     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 101        |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00677   |\n",
      "|    value_loss           | 251        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 287          |\n",
      "|    ep_rew_mean          | -41.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1178         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118731605 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 31.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 304         |\n",
      "|    ep_rew_mean          | -41.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1179        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008874142 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 318         |\n",
      "|    ep_rew_mean          | -41.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1180        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011502445 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.3        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "Final timesteps: 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olcan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-hovering model performansı: -306.54\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "class AntiHoveringLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.hovering_count = 0\n",
    "        self.step_count = 0\n",
    "        self.altitude_history = []\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.hovering_count = 0\n",
    "        self.step_count = 0\n",
    "        self.altitude_history = []\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy, angle, angular_vel, leg1, leg2 = obs\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Altitude history tutla\n",
    "            self.altitude_history.append(y)\n",
    "            if len(self.altitude_history) > 20:\n",
    "                self.altitude_history.pop(0)\n",
    "            \n",
    "            # AGRESIF HOVERING DETECTION\n",
    "            is_hovering = (\n",
    "                y > 0.15 and                    # Yeterince yüksekte\n",
    "                velocity_mag < 0.08 and         # Çok yavaş\n",
    "                abs(vx) < 0.05 and             # Yatay hareket az\n",
    "                abs(vy) < 0.05                 # Dikey hareket az\n",
    "            )\n",
    "            \n",
    "            if is_hovering:\n",
    "                self.hovering_count += 1\n",
    "                \n",
    "                # Hovering süresine göre artan ceza\n",
    "                base_penalty = -0.1\n",
    "                time_penalty = -0.02 * self.hovering_count\n",
    "                \n",
    "                # Uzun süre hovering = episode sonlandır\n",
    "                if self.hovering_count > 30:  # 30 step = yaklaşık 1 saniye\n",
    "                    reward = -200  # Büyük ceza\n",
    "                    terminated = True\n",
    "                    info['hovering_termination'] = True\n",
    "                else:\n",
    "                    reward += base_penalty + time_penalty\n",
    "                \n",
    "            else:\n",
    "                self.hovering_count = max(0, self.hovering_count - 2)  # Yavaşça azalt\n",
    "            \n",
    "            # FUEL AGRESIF CEZA\n",
    "            if action == 2:  # Ana motor\n",
    "                reward -= 0.03\n",
    "            elif action in [1, 3]:  # Yan motorlar\n",
    "                reward -= 0.015\n",
    "            \n",
    "            # PROGRESS ZORLAMA\n",
    "            target_x = 0.0\n",
    "            distance_to_target = abs(x - target_x)\n",
    "            \n",
    "            # Çok uzak kalırsa ceza\n",
    "            if distance_to_target > 0.8:\n",
    "                reward -= 0.05\n",
    "            \n",
    "            # Aşağı gitmeyi teşvik et\n",
    "            if len(self.altitude_history) >= 10:\n",
    "                recent_avg = np.mean(self.altitude_history[-10:])\n",
    "                older_avg = np.mean(self.altitude_history[-20:-10]) if len(self.altitude_history) >= 20 else recent_avg\n",
    "                \n",
    "                if recent_avg < older_avg:  # Aşağı iniyor\n",
    "                    reward += 0.05\n",
    "                elif recent_avg > older_avg:  # Yukarı çıkıyor\n",
    "                    reward -= 0.03\n",
    "            \n",
    "            # MAX EPISODE LIMIT\n",
    "            if self.step_count > 500:  # Normal limit 1000, biz 500 yapalım\n",
    "                reward -= 0.1\n",
    "                if self.step_count > 600:\n",
    "                    terminated = True\n",
    "                    info['time_limit_termination'] = True\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "class ShapedLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "            \n",
    "            # Hovering penalty\n",
    "            if y > 0.1 and velocity_mag < 0.05:\n",
    "                reward -= 0.02\n",
    "            \n",
    "            # Fuel penalty\n",
    "            if action == 2:  # Main engine\n",
    "                reward -= 0.01\n",
    "            elif action in [1, 3]:  # Side engines\n",
    "                reward -= 0.005\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# 25K modelinin üzerine shaped env ile eğitim\n",
    "def train_on_shaped_env():\n",
    "    # Shaped environment oluştur\n",
    "    shaped_env = ShapedLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # 25K modelini yükle\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_100000_model\", env=shaped_env)\n",
    "    \n",
    "    print(f\"Başlangıç timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # 25K step daha eğit\n",
    "    model.learn(total_timesteps=200000, reset_num_timesteps=False)  # 10K + 25K = 35K\n",
    "    \n",
    "    print(f\"Final timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # Test\n",
    "    mean_reward, _ = evaluate_policy(model, shaped_env, n_eval_episodes=5)\n",
    "    print(f\"Shaped env performansı: {mean_reward:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    model.save(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    return model\n",
    "\n",
    "def test_anti_hovering():\n",
    "    \"\"\"Anti-hovering environment'ları test et\"\"\"\n",
    "    \n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    \n",
    "    # 1. Normal shaped env (sizin mevcut)\n",
    "    shaped_env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # 2. Force descent env\n",
    "    force_env = ForceDescentLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    print(\"=== ANTI-HOVERING TEST ===\")\n",
    "    \n",
    "    # Mevcut modelinizi test edin\n",
    "    model_path = r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\"\n",
    "    \n",
    "    try:\n",
    "        # Shaped env ile test\n",
    "        model1 = DQN.load(model_path, env=shaped_env)\n",
    "        perf1, _ = evaluate_policy(model1, shaped_env, n_eval_episodes=3)\n",
    "        print(f\"Anti-hovering env: {perf1:.2f}\")\n",
    "        \n",
    "        # Force env ile test\n",
    "        model2 = DQN.load(model_path, env=force_env)\n",
    "        perf2, _ = evaluate_policy(model2, force_env, n_eval_episodes=3)\n",
    "        print(f\"Force descent env: {perf2:.2f}\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Model yüklenemedi, yeni model eğitiliyor...\")\n",
    "        \n",
    "        # Anti-hovering ile eğitim\n",
    "        model = DQN(\"MlpPolicy\", shaped_env, verbose=1)\n",
    "        model.learn(total_timesteps=20000)\n",
    "        \n",
    "        perf, _ = evaluate_policy(model, shaped_env, n_eval_episodes=5)\n",
    "        print(f\"Yeni anti-hovering model: {perf:.2f}\")\n",
    "        \n",
    "        model.save(\"anti_hovering_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    force_env.close()\n",
    "class ForceDescentLunarLander(gym.Wrapper):\n",
    "    \"\"\"Daha da agresif - iniş zorlaması\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.step_count = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.prev_obs is not None:\n",
    "            x, y, vx, vy = obs[:4]\n",
    "            prev_y = self.prev_obs[1]\n",
    "            \n",
    "            # BASIT AMA ETKİLİ: Step sayısına göre ceza\n",
    "            altitude_penalty = -0.01 * self.step_count * y\n",
    "            reward += altitude_penalty\n",
    "            \n",
    "            # Aşağı gitme ödülü\n",
    "            if y < prev_y:  # Aşağı iniyor\n",
    "                reward += 0.1 * (prev_y - y)\n",
    "            \n",
    "            # Yukarı gitme cezası\n",
    "            elif y > prev_y:  # Yukarı çıkıyor\n",
    "                reward -= 0.2 * (y - prev_y)\n",
    "            \n",
    "            # Yükseklik threshold'u\n",
    "            if y > 1.0 and self.step_count > 100:\n",
    "                reward -= 1.0  # Çok yüksekte çok uzun süre\n",
    "            \n",
    "            # Episode sonlandırma\n",
    "            if self.step_count > 400:\n",
    "                terminated = True\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "def fresh_shaped_training():\n",
    "    \"\"\"Sıfırdan shaped environment ile eğitim\"\"\"\n",
    "    \n",
    "    print(\"\\n=== SIFIRDAN SHAPED EĞİTİMİ ===\")\n",
    "    \n",
    "    shaped_env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # Yeni model oluştur\n",
    "    fresh_model = DQN(\"MlpPolicy\", shaped_env, verbose=1)\n",
    "    \n",
    "    # Eğit\n",
    "    fresh_model.learn(total_timesteps=150000)\n",
    "    \n",
    "    # Test\n",
    "    fresh_perf, _ = evaluate_policy(fresh_model, shaped_env, n_eval_episodes=5)\n",
    "    print(f\"Sıfırdan shaped model performansı: {fresh_perf:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    fresh_model.save(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_shaped_model\")\n",
    "    \n",
    "    shaped_env.close()\n",
    "    return fresh_model, fresh_perf\n",
    "\n",
    "def quick_fix():\n",
    "    \"\"\"Hızlı hovering düzeltmesi\"\"\"\n",
    "    \n",
    "    # Environment\n",
    "    env = AntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    \n",
    "    # Model yükle (yeni model oluşturmaya gerek yok)\n",
    "    #model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\PPO\\final_anti_hovering_model\", env=env)\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,        # PPO için tipik\n",
    "        n_steps=2048,              # rollout buffer length\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "    )\n",
    "\n",
    "    print(f\"Başlangıç timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    model.learn(total_timesteps=50000, reset_num_timesteps=False)\n",
    "    \n",
    "    print(f\"Final timesteps: {model.num_timesteps}\")\n",
    "    \n",
    "    # Test\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    perf, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"Anti-hovering model performansı: {perf:.2f}\")\n",
    "    \n",
    "    # Kaydet\n",
    "    model.save(\"PPO\\AntiHoveringLunarLander_model\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Çalıştır\n",
    "model = quick_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "151fa76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEÇENEKLER:\n",
      "1. nuclear_option()      - EN EXTREME ÇÖZÜM\n",
      "2. simple_brutal_fix()   - BASIT BRUTAL\n",
      "3. debug_hovering()      - MEVCUT MODELİ DEBUG ET\n",
      "🔍 HOVERING DEBUG\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step  0: Action=2, Y=1.386, Vel=0.739, Reward=1.31\n",
      "Step  1: Action=2, Y=1.375, Vel=0.718, Reward=2.40\n",
      "Step  2: Action=2, Y=1.363, Vel=0.718, Reward=0.32\n",
      "Step  3: Action=2, Y=1.352, Vel=0.708, Reward=1.12\n",
      "Step  4: Action=2, Y=1.341, Vel=0.672, Reward=3.68\n",
      "Step  5: Action=2, Y=1.331, Vel=0.646, Reward=2.55\n",
      "Step  6: Action=2, Y=1.322, Vel=0.633, Reward=1.16\n",
      "Step  7: Action=2, Y=1.312, Vel=0.644, Reward=-1.20\n",
      "Step  8: Action=2, Y=1.303, Vel=0.626, Reward=1.61\n",
      "Step  9: Action=2, Y=1.294, Vel=0.639, Reward=-1.37\n",
      "Step 10: Action=2, Y=1.285, Vel=0.647, Reward=-0.85\n",
      "Step 11: Action=2, Y=1.277, Vel=0.646, Reward=-0.01\n",
      "Step 12: Action=1, Y=1.268, Vel=0.654, Reward=-0.39\n",
      "Step 13: Action=2, Y=1.260, Vel=0.668, Reward=-1.12\n",
      "Step 14: Action=1, Y=1.251, Vel=0.673, Reward=0.27\n",
      "Step 15: Action=2, Y=1.242, Vel=0.672, Reward=0.60\n",
      "Step 16: Action=1, Y=1.232, Vel=0.680, Reward=0.21\n",
      "Step 17: Action=2, Y=1.223, Vel=0.673, Reward=1.45\n",
      "Step 18: Action=2, Y=1.213, Vel=0.675, Reward=0.48\n",
      "Step 19: Action=2, Y=1.205, Vel=0.656, Reward=2.50\n",
      "Step 20: Action=1, Y=1.195, Vel=0.665, Reward=0.21\n",
      "Step 21: Action=2, Y=1.186, Vel=0.648, Reward=2.40\n",
      "Step 22: Action=1, Y=1.177, Vel=0.656, Reward=0.46\n",
      "Step 23: Action=2, Y=1.168, Vel=0.657, Reward=0.88\n",
      "Step 24: Action=2, Y=1.159, Vel=0.648, Reward=1.87\n",
      "Step 25: Action=1, Y=1.150, Vel=0.657, Reward=0.46\n",
      "Step 26: Action=2, Y=1.141, Vel=0.654, Reward=1.39\n",
      "Step 27: Action=1, Y=1.132, Vel=0.663, Reward=0.84\n",
      "Step 28: Action=2, Y=1.124, Vel=0.664, Reward=1.30\n",
      "Step 29: Action=2, Y=1.115, Vel=0.672, Reward=0.60\n",
      "Step 30: Action=2, Y=1.107, Vel=0.665, Reward=2.12\n",
      "Step 31: Action=1, Y=1.098, Vel=0.670, Reward=1.47\n",
      "Step 32: Action=2, Y=1.091, Vel=0.642, Reward=4.02\n",
      "Step 33: Action=2, Y=1.084, Vel=0.613, Reward=2.06\n",
      "Step 34: Action=2, Y=1.077, Vel=0.589, Reward=1.52\n",
      "Step 35: Action=1, Y=1.071, Vel=0.592, Reward=-1.03\n",
      "Step 36: Action=2, Y=1.064, Vel=0.569, Reward=1.29\n",
      "Step 37: Action=2, Y=1.058, Vel=0.549, Reward=0.88\n",
      "Step 38: Action=2, Y=1.053, Vel=0.547, Reward=-0.97\n",
      "Step 39: Action=2, Y=1.048, Vel=0.536, Reward=-0.07\n",
      "Step 40: Action=2, Y=1.044, Vel=0.518, Reward=0.50\n",
      "Step 41: Action=2, Y=1.040, Vel=0.503, Reward=0.21\n",
      "Step 42: Action=2, Y=1.036, Vel=0.491, Reward=-0.18\n",
      "Step 43: Action=2, Y=1.033, Vel=0.489, Reward=-1.30\n",
      "Step 44: Action=2, Y=1.031, Vel=0.480, Reward=-0.74\n",
      "Step 45: Action=1, Y=1.029, Vel=0.476, Reward=-1.16\n",
      "Step 46: Action=2, Y=1.026, Vel=0.459, Reward=-0.02\n",
      "Step 47: Action=2, Y=1.024, Vel=0.445, Reward=-0.38\n",
      "Step 48: Action=2, Y=1.022, Vel=0.419, Reward=0.89\n",
      "Step 49: Action=2, Y=1.021, Vel=0.396, Reward=0.55\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "class ExtremeAntiHoveringLunarLander(gym.Wrapper):\n",
    "    \"\"\"HOVERING = INSTANT DEATH\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_obs = None\n",
    "        self.hovering_steps = 0\n",
    "        self.step_count = 0\n",
    "        self.max_altitude_seen = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        self.hovering_steps = 0\n",
    "        self.step_count = 0\n",
    "        self.max_altitude_seen = obs[1]\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        x, y, vx, vy, angle, angular_vel, leg1, leg2 = obs\n",
    "        velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        # Max altitude güncelle\n",
    "        self.max_altitude_seen = max(self.max_altitude_seen, y)\n",
    "        \n",
    "        # HOVERING DETECTION (çok hassas)\n",
    "        is_hovering = (\n",
    "            y > 0.1 and \n",
    "            velocity_mag < 0.1 and \n",
    "            abs(vy) < 0.03\n",
    "        )\n",
    "        \n",
    "        if is_hovering:\n",
    "            self.hovering_steps += 1\n",
    "            \n",
    "            # SADECE 5 STEP HOVERING İZİN VER\n",
    "            if self.hovering_steps > 5:\n",
    "                reward = -500  # EXTREME PENALTY\n",
    "                terminated = True\n",
    "                info['hovering_death'] = True\n",
    "                print(f\"🚨 HOVERING DEATH at step {self.step_count}\")\n",
    "        else:\n",
    "            self.hovering_steps = 0\n",
    "        \n",
    "        # ALTITUDE-BASED EXTREME PENALTIES\n",
    "        if y > 0.5:\n",
    "            reward -= 2.0  # Çok yüksekte olmak = büyük ceza\n",
    "        \n",
    "        if y > 0.8:\n",
    "            reward -= 5.0  # Aşırı yüksekte = çok büyük ceza\n",
    "        \n",
    "        # YUKAR ÇIKMA = ÖLÜM\n",
    "        if self.prev_obs is not None:\n",
    "            prev_y = self.prev_obs[1]\n",
    "            if y > prev_y + 0.02:  # Yukarı çıkıyor\n",
    "                reward -= 10.0\n",
    "                print(f\"⬆️ UPWARD MOVEMENT PENALTY: {y:.3f} -> {prev_y:.3f}\")\n",
    "        \n",
    "        # PROGRESS FORCE (aşağı gitmeyi zorla)\n",
    "        if self.prev_obs is not None:\n",
    "            prev_y = self.prev_obs[1]\n",
    "            if y < prev_y:  # Aşağı iniyor\n",
    "                reward += 5.0 * (prev_y - y)  # Büyük ödül\n",
    "        \n",
    "        # TIME PRESSURE (zamanla artan ceza)\n",
    "        time_penalty = -0.1 * (self.step_count / 100)\n",
    "        reward += time_penalty\n",
    "        \n",
    "        # EPISODE HARD LIMIT\n",
    "        if self.step_count > 200:  # Çok kısa episode\n",
    "            reward = -1000\n",
    "            terminated = True\n",
    "            info['time_limit_death'] = True\n",
    "        \n",
    "        # ALTITUDE REGRESSION REWARD\n",
    "        altitude_progress = self.max_altitude_seen - y\n",
    "        if altitude_progress > 0:\n",
    "            reward += altitude_progress * 10  # Ne kadar aşağı indiyse o kadar ödül\n",
    "        \n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "class ForcedDescentLunarLander(gym.Wrapper):\n",
    "    \"\"\"Baştan aşağı inmeye zorla\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.step_count = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        y = obs[1]  # altitude\n",
    "        \n",
    "        # BASIT BRUTAL APPROACH: Altitude = instant penalty\n",
    "        altitude_penalty = -y * 10  # Her metre yükseklik = -10 reward\n",
    "        reward += altitude_penalty\n",
    "        \n",
    "        # Step penalty\n",
    "        reward -= 0.1\n",
    "        \n",
    "        # Hard time limit\n",
    "        if self.step_count > 150:\n",
    "            terminated = True\n",
    "            reward = -500\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "def nuclear_option():\n",
    "    \"\"\"NUCLEAR OPTION: Sıfırdan tamamen yeni approach\"\"\"\n",
    "    \n",
    "    print(\"🚨 NUCLEAR OPTION: EXTREME ANTI-HOVERING 🚨\")\n",
    "    \n",
    "    # En extreme environment\n",
    "    env = ExtremeAntiHoveringLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\fresh_100000_model\", env=env)\n",
    "\n",
    "    model.learn(total_timesteps=200000, reset_num_timesteps=False)  # 10K + 25K = 35K\n",
    "    # Yeni model - sıfırdan başla\n",
    "    # model = DQN(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     env, \n",
    "    #     verbose=1,\n",
    "    #     learning_rate=1e-3,\n",
    "    #     exploration_initial_eps=1.0,    # Tam exploration\n",
    "    #     exploration_final_eps=0.01,     # Minimum exploration  \n",
    "    #     exploration_fraction=0.5,       # Yarıya kadar explore\n",
    "    #     target_update_interval=500,     # Daha sık update\n",
    "    #     train_freq=1,                   # Her step train\n",
    "    #     buffer_size=10000               # Küçük buffer\n",
    "    # )\n",
    "    \n",
    "    print(\"🔥 EXTREME TRAINING BAŞLIYOR...\")\n",
    "    \n",
    "    # Çok kısa ama yoğun eğitim\n",
    "    #model.learn(total_timesteps=100000)\n",
    "    \n",
    "    print(\"✅ EXTREME TRAINING BİTTİ\")\n",
    "    \n",
    "    # Test\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    \n",
    "    print(\"🧪 TESTING...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"EXTREME Model Performance: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    \n",
    "    # Manuel test\n",
    "    print(\"\\n🎮 MANUEL TEST:\")\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while steps < 300:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        y = obs[1]\n",
    "        print(f\"Step {steps}: Action={action}, Altitude={y:.3f}, Reward={reward:.2f}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Manuel test sonucu: {total_reward:.2f} in {steps} steps\")\n",
    "    \n",
    "    if 'hovering_death' in info:\n",
    "        print(\"💀 HOVERING DEATH!\")\n",
    "    elif 'time_limit_death' in info:\n",
    "        print(\"⏰ TIME LIMIT DEATH!\")\n",
    "    elif terminated:\n",
    "        print(\"🎯 MISSION COMPLETE!\")\n",
    "    \n",
    "    # Son çare kaydet\n",
    "    model.save(\"nuclear_anti_hovering_model\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def simple_brutal_fix():\n",
    "    \"\"\"En basit brutal çözüm\"\"\"\n",
    "    \n",
    "    print(\"💥 SIMPLE BRUTAL FIX\")\n",
    "    \n",
    "    env = ForcedDescentLunarLander(gym.make(\"LunarLander-v3\"))\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\brutal_model\", env=env)\n",
    "    #model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=100000, reset_num_timesteps=False)\n",
    "    \n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    perf, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    print(f\"Brutal model: {perf:.2f}\")\n",
    "    \n",
    "    model.save(\"brutal_model2\")\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# SON ÇARE TEST\n",
    "def debug_hovering():\n",
    "    \"\"\"Hovering davranışını debug et\"\"\"\n",
    "    \n",
    "    print(\"🔍 HOVERING DEBUG\")\n",
    "    \n",
    "    # Mevcut modelinizi yükle\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    model = DQN.load(r\"E:\\UNITY\\BombermanTower\\python\\Jupyter\\shaped_model\", env=env)\n",
    "    \n",
    "    # 10 step takip et\n",
    "    obs, _ = env.reset()\n",
    "    for step in range(50):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        x, y, vx, vy = obs[:4]\n",
    "        velocity = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        print(f\"Step {step:2d}: Action={action}, Y={y:.3f}, Vel={velocity:.3f}, Reward={reward:.2f}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# KULLANIM:\n",
    "print(\"SEÇENEKLER:\")\n",
    "print(\"1. nuclear_option()      - EN EXTREME ÇÖZÜM\")\n",
    "print(\"2. simple_brutal_fix()   - BASIT BRUTAL\")  \n",
    "print(\"3. debug_hovering()      - MEVCUT MODELİ DEBUG ET\")\n",
    "\n",
    "# EN EXTREME ÇÖZÜM:\n",
    "model = debug_hovering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
