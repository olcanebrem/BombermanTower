# Bomberman PPO Training Configuration

# File paths
paths:
  model_dir: "models/"
  log_dir: "logs/"
  checkpoint_dir: "checkpoints/"
  unity_env_path: null  # Set to Unity build path, null for editor

# Unity Environment Settings
unity:
  env_path: null  # Path to Unity build executable (null to use editor)
  time_scale: 20.0  # Speed up training (1.0 for normal speed)
  no_graphics: true  # Disable graphics for faster training

# PPO Hyperparameters
ppo:
  learning_rate: 3.0e-4  # Learning rate
  n_steps: 2048          # Number of steps per environment per update
  batch_size: 64         # Batch size for training
  n_epochs: 10           # Number of epochs for each update
  gamma: 0.99            # Discount factor
  gae_lambda: 0.95       # GAE lambda parameter
  clip_range: 0.2        # PPO clipping parameter
  clip_range_vf: null    # Clipping parameter for value function (null = same as clip_range)
  normalize_advantage: true  # Whether to normalize advantages
  ent_coef: 0.01         # Entropy coefficient
  vf_coef: 0.5           # Value function coefficient
  max_grad_norm: 0.5     # Maximum gradient norm
  use_sde: false         # Use State Dependent Exploration
  sde_sample_freq: -1    # Sample frequency for SDE
  target_kl: null        # Target KL divergence (null = no early stopping)

# Training Settings
training:
  total_timesteps: 2000000  # Total training timesteps
  n_envs: 8                 # Number of parallel environments
  device: "auto"            # Training device (auto, cpu, cuda)
  no_graphics: true         # Disable graphics during training
  seed: 42                  # Random seed for reproducibility

# Environment Wrapper Settings
wrapper:
  frame_stack: 1            # Number of frames to stack
  max_episode_steps: 3000   # Maximum steps per episode
  reward_shaping: true      # Enable reward shaping
  normalize_observations: false  # Normalize observations
  
# Evaluation Settings
evaluation:
  enabled: true             # Enable periodic evaluation
  eval_freq: 50000          # Evaluation frequency (in timesteps)
  n_eval_episodes: 10       # Number of episodes per evaluation
  deterministic: true       # Use deterministic policy for evaluation

# Checkpoint Settings
checkpoints:
  enabled: true             # Enable periodic checkpoints
  save_freq: 100000         # Checkpoint frequency (in timesteps)
  keep_last_n: 5            # Number of recent checkpoints to keep

# Curriculum Learning
curriculum:
  enabled: true             # Enable curriculum learning
  
  # Level progression thresholds
  levels:
    beginner:
      min_success_rate: 0.0   # Start immediately
      enemy_count: 1          # Fewer enemies
      map_size: [11, 11]      # Smaller map
      collectible_count: 3    # Fewer collectibles
      breakable_wall_density: 0.2
      
    intermediate:
      min_success_rate: 0.3   # 30% success rate to advance
      enemy_count: 2
      map_size: [13, 13]
      collectible_count: 4
      breakable_wall_density: 0.25
      
    advanced:
      min_success_rate: 0.5   # 50% success rate to advance
      enemy_count: 3
      map_size: [15, 15]      # Full size
      collectible_count: 5
      breakable_wall_density: 0.3
      
    expert:
      min_success_rate: 0.7   # 70% success rate to advance
      enemy_count: 4
      map_size: [15, 15]
      collectible_count: 6
      breakable_wall_density: 0.35

  # Curriculum evaluation settings
  evaluation_window: 100      # Episodes to evaluate for progression
  update_frequency: 25000     # How often to check progression (timesteps)

# Reward Shaping Parameters
rewards:
  # Primary rewards
  level_complete: 10.0
  enemy_kill: 2.0
  collectible_base: 1.0
  health_collectible: 1.5
  upgrade_collectible: 2.0
  
  # Action rewards
  bomb_placed: 0.1
  wall_destroyed: 0.2
  exploration: 0.05
  
  # Penalties
  death: -5.0
  damage: -1.0
  wall_collision: -0.1
  inactivity: -0.02
  step_penalty: -0.001
  timeout: -2.0
  bomb_self_damage: -0.5
  
  # Distance-based rewards
  enemy_proximity: 0.01
  exit_proximity: 0.02
  collectible_proximity: 0.01
  
  # Strategic rewards
  tactical_positioning: 0.05
  safety_bonus: 0.01
  all_enemies_cleared: 3.0

# Logging and Monitoring
logging:
  tensorboard: true         # Enable TensorBoard logging
  console_level: "INFO"     # Console logging level
  file_level: "DEBUG"       # File logging level
  log_rewards: true         # Log individual rewards
  log_actions: false        # Log action distributions
  log_observations: false   # Log observation statistics

# Performance Optimization
performance:
  multiprocessing: true     # Use multiprocessing for parallel envs
  worker_timeout: 60        # Worker timeout in seconds
  memory_limit_gb: 8        # Memory limit per worker
  
# Advanced Settings
advanced:
  # Policy network architecture
  policy_layers: [256, 256]  # Hidden layer sizes
  value_layers: [256, 256]   # Value network hidden layers
  activation: "tanh"         # Activation function
  
  # Observation space settings
  observation_normalization: false
  observation_clipping: false
  
  # Action space settings
  action_masking: false      # Enable invalid action masking
  
  # Exploration settings
  exploration_schedule:
    initial_epsilon: 0.1
    final_epsilon: 0.01
    decay_steps: 500000

# Experimental Features
experimental:
  self_play: false          # Enable self-play training
  hindsight_experience_replay: false  # Enable HER
  curiosity_driven_exploration: false  # Enable ICM
  
# Hardware Specific Settings
hardware:
  # GPU settings
  gpu_memory_growth: true   # Allow GPU memory growth
  mixed_precision: false    # Use mixed precision training
  
  # CPU settings
  cpu_cores: -1             # Number of CPU cores (-1 = all available)
  
# Debug Settings
debug:
  verbose_training: false   # Verbose training output
  save_replay_buffer: false # Save replay buffer for analysis
  profile_performance: false # Profile training performance
  validate_environment: true # Validate environment setup